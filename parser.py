import pdfplumber
import pandas as pd
import re
import os
import glob
import shutil
import time

# ==========================================
# 1. ç³»çµ±åƒæ•¸èˆ‡è·¯å¾‘è¨­å®š
# ==========================================
INPUT_DIR = "./inpdf"        # å¾…è™•ç†æª”æ¡ˆç›®éŒ„
PROCESSED_DIR = "./overpdf"  # è™•ç†å®Œæˆæª”æ¡ˆç§»å…¥ç›®éŒ„
OUTPUT_CSV = "Batch_Import_Declarations.csv" # æœ€çµ‚å½™æ•´çš„ CSV æª”å

# --- ä»¥ä¸‹æ ¸å¿ƒåƒæ•¸ç¶­æŒ V12.0 é‚è¼¯ä¸è®Š ---
COORD_ITEM_MAX_X = 50
COORD_DESC_MIN_X = 10
COORD_DESC_MAX_X = 202
COORD_SPLIT_CCC = 202
COORD_NOISE_START = 315

GLOBAL_IGNORE_KEYWORDS = [
    "å ±å–®è™Ÿç¢¼", "ä¸»æå–®è™Ÿç¢¼", "ç”Ÿç”¢åœ‹åˆ¥", "è¼¸å‡ºå…¥è¨±å¯æ–‡ä»¶è™Ÿç¢¼", 
    "è¼¸å‡ºå…¥è²¨å“åˆ†é¡è™Ÿåˆ—", "ç´ç¨…è¾¦æ³•", "è²¨ç‰©åç¨±", "å“ç‰Œ", "è¦æ ¼",
    "æ•¸é‡", "å–®ä½", "æ·¨é‡", "å–®åƒ¹", "å¹£åˆ¥", "FCL/FCL", "åŒ…è£èªªæ˜",
    "TOTAL", "PAGE", "TERM OF", "é€²å£å ±å–®", "é … æ¬¡", "æ¨™è¨˜", 
    "è²¨æ«ƒè™Ÿç¢¼", "å…¶ä»–ç”³å ±äº‹é …", "é•·æœŸå§”ä»»", "æœªæŠ•ä¿", "WHSU", "0CTN"
]

# ==========================================
# 2. æ ¸å¿ƒé‚è¼¯å‡½å¼ (ç¶­æŒ V12.0 ä¸è®Š)
# ==========================================

def is_header_noise(text):
    if not text: return False
    clean_t = text.replace(" ", "")
    for kw in GLOBAL_IGNORE_KEYWORDS:
        if kw.replace(" ", "") in clean_t:
            return True
    return False

def extract_ccc_permit(raw_ccc_list):
    raw_text = "".join(raw_ccc_list)
    normalized = re.sub(r'[^A-Z0-9]', '', raw_text)
    
    permit_val = ""
    ccc_val = ""
    
    match_p = re.search(r'(CI\d{12}|IFB[A-Z0-9]{11})', normalized)
    if match_p:
        permit_val = match_p.group(1)
        normalized = normalized.replace(permit_val, '', 1)
        
    match_c = re.search(r'(\d{10,11})', normalized)
    if match_c:
        raw_ccc = match_c.group(1)
        if len(raw_ccc) == 11:
            ccc_val = f"{raw_ccc[:4]}.{raw_ccc[4:6]}.{raw_ccc[6:8]}.{raw_ccc[8:10]}-{raw_ccc[10]}"
        else:
            ccc_val = f"{raw_ccc[:4]}.{raw_ccc[4:6]}.{raw_ccc[6:8]}.{raw_ccc[8:10]}"
            
    return ccc_val, permit_val

def extract_country_and_clean_desc(desc_list):
    full_desc = " ".join(desc_list)
    country_val = ""
    
    country_match = re.search(r"\b([A-Z]+(?:\s+[A-Z]+)*)\s+([A-Z]{2})\b", full_desc)
    valid_codes = ['TH', 'CN', 'JP', 'US', 'VN', 'TW', 'KR', 'ID', 'MY', 'DE', 'IT', 'FR', 'GB']
    
    if country_match:
        found_code = country_match.group(2)
        if found_code in valid_codes:
            country_val = country_match.group(0)
            full_desc = full_desc.replace(country_val, " ")
            
    full_desc = re.sub(r"\b\d{13}\b", " ", full_desc)
    full_desc = re.sub(r"\b(FOB|JPY|KGM|PCE)\b", " ", full_desc)
    full_desc = full_desc.strip()
    full_desc = re.sub(r"^[\d\.\-\s]+", "", full_desc)
    full_desc = re.sub(r"\s+", " ", full_desc).strip()
    
    return full_desc, country_val

def generate_sop(ccc, permit):
    notes = []
    p = str(permit)
    c = str(ccc).replace(".", "").replace("-", "")
    
    if 'IFB' in p: notes.append("é£Ÿå“å®¹å™¨ (Food Contact) - éœ€æª¢é©—")
    elif 'CI' in p: notes.append("ä¸€èˆ¬æŸ¥é©— (General Inspection)")
    elif 'DH' in p: notes.append("å¯èƒ½ç‚ºå…é©—æˆ–æ ¸å‚™ä»£ç¢¼")
    
    if c.startswith('9503'): notes.append("ç©å…· (Toys) - éœ€ BSMI æª¢é©—")
    if c.startswith('3924'): notes.append("å¡‘è† /ç¾è€çš¿æª¢é©—")
    if c.startswith('940'): notes.append("ç‡ˆå…·/å®¶å…· - æ³¨æ„æª¢é©—")
    if c.startswith('691'): notes.append("é™¶ç“·æª¢é©—")
    if c.startswith('9603'): notes.append("åˆ·å…· - æ³¨æ„å‹•ç‰©æ¯›/æ¤ç‰©æ¯›")
    if c.startswith('630') or c.startswith('570'): notes.append("ç´¡ç¹”å“ - æ³¨æ„æˆåˆ†æ¨™ç¤º")
    if c.startswith('910'): notes.append("é˜éŒ¶/è¨ˆæ™‚å™¨ - æ³¨æ„é›»æ± è¦å®š")
    
    return "ï¼›".join(notes)

# ==========================================
# 3. å–®ä¸€æª”æ¡ˆè§£æå¼•æ“ (V12.0 é‚è¼¯)
# ==========================================

def parse_single_pdf(pdf_path):
    # é€™è£¡å®Œå…¨ä¿ç•™ V12.0 çš„æ ¸å¿ƒè§£ææµç¨‹
    items = [] 
    last_item_idx = None 
    decl_no = "Unknown"

    try:
        with pdfplumber.open(pdf_path) as pdf:
            # æŠ“å ±å–®è™Ÿ
            p1_text = pdf.pages[0].extract_text() or ""
            decl_match = re.search(r"([A-Z]{2}/[\s\d/]+/[A-Z0-9]+)", p1_text)
            if decl_match: 
                decl_no = decl_match.group(1).replace(" ", "").replace("//", "/")

            for page_num, page in enumerate(pdf.pages):
                words = page.extract_words(keep_blank_chars=True)
                
                valid_words = []
                for w in words:
                    if not is_header_noise(w['text']):
                        valid_words.append(w)
                words = valid_words

                anchors = []
                for w in words:
                    if w['x0'] < COORD_SPLIT_CCC: 
                        if re.match(r"^\d+\.$", w['text'].strip()):
                            item_num = int(w['text'].strip().replace(".", ""))
                            anchors.append({'item': item_num, 'top': w['top']})
                anchors.sort(key=lambda x: x['top'])
                
                zones = []
                if anchors:
                    first_anchor_top = anchors[0]['top']
                    if first_anchor_top > 10: 
                        zones.append({'start_y': 0, 'end_y': first_anchor_top, 'item_id': last_item_idx})
                else:
                    zones.append({'start_y': 0, 'end_y': page.height, 'item_id': last_item_idx})
                
                for i in range(len(anchors)):
                    start_y = anchors[i]['top']
                    end_y = anchors[i+1]['top'] if i < len(anchors) - 1 else page.height
                    last_item_idx = anchors[i]['item']
                    zones.append({'start_y': start_y, 'end_y': end_y, 'item_id': anchors[i]['item']})
                
                for zone in zones:
                    z_item_id = zone['item_id']
                    if z_item_id is None: continue 
                    
                    target_item = next((it for it in items if it['item_no'] == z_item_id), None)
                    if not target_item:
                        target_item = {'item_no': z_item_id, 'desc_parts': [], 'ccc_parts': [], 'decl_no': decl_no}
                        items.append(target_item)
                    
                    zone_words = [w for w in words if zone['start_y'] <= w['top'] < zone['end_y']]
                    zone_words.sort(key=lambda w: (round(w['top']/2), w['x0']))
                    
                    for w in zone_words:
                        x = w['x0']
                        text = w['text']
                        
                        if COORD_DESC_MIN_X <= x < COORD_SPLIT_CCC:
                            if re.match(r"^\d+\.$", text.strip()): continue 
                            target_item['desc_parts'].append(text)
                            
                        elif COORD_SPLIT_CCC <= x < COORD_NOISE_START:
                            target_item['ccc_parts'].append(text)

        # æ•´ç†çµæœ
        final_data = []
        items.sort(key=lambda x: x['item_no'])
        
        for it in items:
            ccc_val, permit_val = extract_ccc_permit(it['ccc_parts'])
            desc_val, country_val = extract_country_and_clean_desc(it['desc_parts'])
            
            barcode = ""
            full_raw_desc = " ".join(it['desc_parts'])
            bc_match = re.search(r"\b(\d{13})\b", full_raw_desc)
            if bc_match: barcode = bc_match.group(1)

            final_data.append({
                "å ±å–®è™Ÿç¢¼": it['decl_no'],
                "é …æ¬¡": it['item_no'],
                "è²¨è™Ÿ/æ¢ç¢¼": barcode,
                "è²¨ç‰©åç¨±": desc_val,
                "ç¨…å‰‡è™Ÿåˆ—": ccc_val,
                "è¨±å¯è­‰è™Ÿç¢¼": permit_val,
                "ç”Ÿç”¢åœ‹åˆ¥": country_val,
                "ç”³å ±æ³¨æ„äº‹é …": generate_sop(ccc_val, permit_val),
                "åŸå§‹æª”å": os.path.basename(pdf_path) # æ–°å¢æª”åæ¬„ä½ä»¥ä¾¿è¿½è¹¤
            })
            
        return final_data

    except Exception as e:
        print(f"âŒ è§£æå¤±æ•—: {os.path.basename(pdf_path)} - åŸå› : {str(e)}")
        return []

# ==========================================
# 4. æ‰¹æ¬¡è™•ç†ä¸»ç¨‹å¼
# ==========================================

def main():
    # A. åˆå§‹åŒ–ç›®éŒ„
    if not os.path.exists(INPUT_DIR):
        os.makedirs(INPUT_DIR)
        print(f"âš ï¸ æ‰¾ä¸åˆ°è¼¸å…¥ç›®éŒ„ï¼Œå·²è‡ªå‹•å»ºç«‹: {INPUT_DIR}")
        print("è«‹å°‡ PDF æª”æ¡ˆæ”¾å…¥è©²ç›®éŒ„å¾Œé‡æ–°åŸ·è¡Œã€‚")
        return

    if not os.path.exists(PROCESSED_DIR):
        os.makedirs(PROCESSED_DIR)
        print(f"ğŸ“ å·²å»ºç«‹è¼¸å‡ºç›®éŒ„: {PROCESSED_DIR}")

    # B. æœå°‹ PDF
    pdf_files = glob.glob(os.path.join(INPUT_DIR, "*.pdf"))
    if not pdf_files:
        print(f"âš ï¸ åœ¨ {INPUT_DIR} ä¸­æ‰¾ä¸åˆ°ä»»ä½• PDF æª”æ¡ˆã€‚")
        return

    print(f"ğŸš€ æ‰¾åˆ° {len(pdf_files)} å€‹æª”æ¡ˆï¼Œé–‹å§‹æ‰¹æ¬¡è™•ç†...")
    
    all_batch_data = []
    success_count = 0
    fail_count = 0

    # C. è¿´åœˆè™•ç†
    for file_path in pdf_files:
        filename = os.path.basename(file_path)
        print(f"   æ­£åœ¨è™•ç†: {filename} ...", end="\r")
        
        # åŸ·è¡Œè§£æ
        file_data = parse_single_pdf(file_path)
        
        # D. åˆ¤æ–·æ˜¯å¦æˆåŠŸ
        if file_data and len(file_data) > 0:
            # æˆåŠŸï¼šåŠ å…¥ç¸½è¡¨
            all_batch_data.extend(file_data)
            success_count += 1
            
            # ç§»å‹•æª”æ¡ˆ (Move)
            try:
                # è‹¥ç›®æ¨™ç›®éŒ„å·²æœ‰åŒåæª”æ¡ˆï¼Œé€™æœƒè¦†è“‹æˆ–å ±éŒ¯ï¼Œè¦–ä½œæ¥­ç³»çµ±è€Œå®š
                # é€™è£¡ä½¿ç”¨ shutil.move
                dst_path = os.path.join(PROCESSED_DIR, filename)
                if os.path.exists(dst_path):
                    os.remove(dst_path) # è‹¥å­˜åœ¨å…ˆåˆªé™¤ï¼Œç¢ºä¿ç§»å‹•æˆåŠŸ
                shutil.move(file_path, dst_path)
            except Exception as e:
                print(f"\nâš ï¸ æª”æ¡ˆç§»å‹•å¤±æ•—: {filename} - {e}")
        else:
            # å¤±æ•—ï¼šä¸ç§»å‹•ï¼Œç•™åœ¨åŸç›®éŒ„
            fail_count += 1
            print(f"\nâŒ ç„¡æ³•æå–è³‡æ–™ (ä¿ç•™åœ¨åŸç›®éŒ„): {filename}")

    print(f"\n\nğŸ“Š æ‰¹æ¬¡è™•ç†å®Œæˆå ±å‘Š:")
    print(f"   âœ… æˆåŠŸç§»è‡³ {PROCESSED_DIR}: {success_count} æª”")
    print(f"   âŒ è§£æå¤±æ•—/ç„¡è³‡æ–™ (ä¿ç•™åœ¨ {INPUT_DIR}): {fail_count} æª”")

    # E. è¼¸å‡º CSV
    if all_batch_data:
        df = pd.DataFrame(all_batch_data)
        
        # æ•´ç†æ¬„ä½
        cols = ['å ±å–®è™Ÿç¢¼', 'é …æ¬¡', 'è²¨è™Ÿ/æ¢ç¢¼', 'è²¨ç‰©åç¨±', 'ç¨…å‰‡è™Ÿåˆ—', 'è¨±å¯è­‰è™Ÿç¢¼', 'ç”Ÿç”¢åœ‹åˆ¥', 'ç”³å ±æ³¨æ„äº‹é …', 'åŸå§‹æª”å']
        df = df[cols]
        
        df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å½™æ•´è³‡æ–™å·²å„²å­˜è‡³: {OUTPUT_CSV}")
    else:
        print("âš ï¸ æœ¬æ¬¡åŸ·è¡Œæ²’æœ‰ç”¢ç”Ÿä»»ä½•æœ‰æ•ˆè³‡æ–™ã€‚")

if __name__ == "__main__":
    main()