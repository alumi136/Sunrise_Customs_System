import pdfplumber
import pandas as pd
import re
import os

# ==========================================
# 1. ç³»çµ±åƒæ•¸è¨­å®š
# ==========================================
PDF_PATH = "./inpdf/G2123æ”¾è¡Œå ±å–®.pdf"
OUTPUT_CSV = "G2123_Corrected.csv"

# ç‰©ç†åº§æ¨™ç•Œç·š (ä¸è®Š)
COORD_ITEM_MAX_X = 50
COORD_DESC_MIN_X = 10       # ç¢ºä¿æŠ“åˆ°å“åé–‹é ­
COORD_SPLIT_CCC = 202       # å·¦æ¬„(å“å)èˆ‡ä¸­æ¬„(ç¨…å‰‡)çš„åˆ†ç•Œ
COORD_NOISE_START = 315     # å³å´é›œè¨Šå€èµ·é»

# å¼·åŠ›è¡¨é ­éæ¿¾åº« (æ–°å¢å¤§é‡å¹²æ“¾è©)
GLOBAL_IGNORE_KEYWORDS = [
    "å ±å–®è™Ÿç¢¼", "ä¸»æå–®è™Ÿç¢¼", "ç”Ÿç”¢åœ‹åˆ¥", "è¼¸å‡ºå…¥è¨±å¯æ–‡ä»¶è™Ÿç¢¼", 
    "è¼¸å‡ºå…¥è²¨å“åˆ†é¡è™Ÿåˆ—", "ç´ç¨…è¾¦æ³•", "è²¨ç‰©åç¨±", "å“ç‰Œ", "è¦æ ¼",
    "æ•¸é‡", "å–®ä½", "æ·¨é‡", "å–®åƒ¹", "å¹£åˆ¥", "FCL/FCL", "åŒ…è£èªªæ˜",
    "TOTAL", "PAGE", "TERM OF", "é€²å£å ±å–®", "é … æ¬¡", "æ¨™è¨˜", 
    "è²¨æ«ƒè™Ÿç¢¼", "å…¶ä»–ç”³å ±äº‹é …", "é•·æœŸå§”ä»»", "æœªæŠ•ä¿", "WHSU", "0CTN"
]

# ==========================================
# 2. æ ¸å¿ƒé‚è¼¯å‡½å¼
# ==========================================

def is_header_noise(text):
    """æª¢æŸ¥æ˜¯å¦ç‚ºè¡¨é ­/é å°¾é›œè¨Š"""
    if not text: return False
    # ç§»é™¤ç©ºç™½å¾Œæª¢æŸ¥ï¼Œé¿å… "é … æ¬¡" é€™ç¨®é–“éš”å¹²æ“¾
    clean_t = text.replace(" ", "")
    for kw in GLOBAL_IGNORE_KEYWORDS:
        if kw.replace(" ", "") in clean_t:
            return True
    return False

def extract_ccc_permit(raw_ccc_list):
    """
    æ™ºæ…§åˆ†é›¢ç¨…å‰‡èˆ‡è¨±å¯è­‰
    é‚è¼¯ï¼šå…ˆæ¨™æº–åŒ– -> æ‹”é™¤è¨±å¯è­‰ -> å‰©ä¸‹çš„å°±æ˜¯ç¨…å‰‡
    """
    # åˆä½µä¸¦æ¨™æº–åŒ– (åªä¿ç•™è‹±æ•¸)
    raw_text = "".join(raw_ccc_list)
    normalized = re.sub(r'[^A-Z0-9]', '', raw_text)
    
    permit_val = ""
    ccc_val = ""
    
    # 1. å„ªå…ˆæå–è¨±å¯è­‰ (CI + 12ç¢¼æ•¸å­— OR IFB + 11ç¢¼è‹±æ•¸)
    # ä½¿ç”¨ search å°‹æ‰¾
    match_p = re.search(r'(CI\d{12}|IFB[A-Z0-9]{11})', normalized)
    if match_p:
        permit_val = match_p.group(1)
        # é—œéµï¼šå¾å­—ä¸²ä¸­ç§»é™¤è¨±å¯è­‰ï¼Œé¿å…å¹²æ“¾ CCC
        normalized = normalized.replace(permit_val, '', 1)
        
    # 2. æå–ç¨…å‰‡ (å‰©ä¸‹çš„å­—ä¸²ä¸­æ‰¾ 10 æˆ– 11 ç¢¼æ•¸å­—)
    match_c = re.search(r'(\d{10,11})', normalized)
    if match_c:
        raw_ccc = match_c.group(1)
        if len(raw_ccc) == 11:
            ccc_val = f"{raw_ccc[:4]}.{raw_ccc[4:6]}.{raw_ccc[6:8]}.{raw_ccc[8:10]}-{raw_ccc[10]}"
        else:
            ccc_val = f"{raw_ccc[:4]}.{raw_ccc[4:6]}.{raw_ccc[6:8]}.{raw_ccc[8:10]}"
            
    return ccc_val, permit_val

def extract_country_and_clean_desc(desc_list):
    """
    æå–ç”Ÿç”¢åœ‹åˆ¥ä¸¦æ¸…æ´—è²¨ç‰©åç¨±
    é‚è¼¯ï¼šåœ¨å“åä¸­æœå°‹åœ‹åˆ¥ç‰¹å¾µ -> æå– -> åˆªé™¤ -> æ¸…æ´—å‰©é¤˜æ–‡å­—
    """
    full_desc = " ".join(desc_list)
    country_val = ""
    
    # 1. æœå°‹ç”Ÿç”¢åœ‹åˆ¥ (ç‰¹å¾µï¼šè‹±æ–‡å–®å­— + 2ç¢¼å¤§å¯«ä»£ç¢¼)
    # ä¾‹å¦‚: THAILAND TH, UNITED STATES US
    # ä½¿ç”¨ Regex å°‹æ‰¾ç¨ç«‹çš„åœ‹åæ¨™ç±¤
    country_match = re.search(r"\b([A-Z]+(?:\s+[A-Z]+)*)\s+([A-Z]{2})\b", full_desc)
    
    # é©—è­‰æ˜¯å¦ç‚ºå¸¸è¦‹åœ‹ç¢¼ (é¿å…èª¤åˆ¤å•†å“å‹è™Ÿ)
    valid_codes = ['TH', 'CN', 'JP', 'US', 'VN', 'TW', 'KR', 'ID', 'MY', 'DE', 'IT', 'FR', 'GB']
    
    if country_match:
        found_code = country_match.group(2)
        if found_code in valid_codes:
            country_val = country_match.group(0) # å®Œæ•´å­—ä¸² "THAILAND TH"
            # å¾æè¿°ä¸­ç§»é™¤
            full_desc = full_desc.replace(country_val, " ")
            
    # 2. æ¸…æ´—å‰©é¤˜çš„è²¨ç‰©åç¨±
    # ç§»é™¤ 13 ç¢¼æ¢ç¢¼
    full_desc = re.sub(r"\b\d{13}\b", " ", full_desc)
    # ç§»é™¤å¸¸è¦‹é›œè¨Š
    full_desc = re.sub(r"\b(FOB|JPY|KGM|PCE)\b", " ", full_desc)
    # ç§»é™¤é–‹é ­çš„éæ–‡å­—ç¬¦è™Ÿ (æ®˜ç•™çš„ 1. æˆ– -)
    full_desc = full_desc.strip()
    full_desc = re.sub(r"^[\d\.\-\s]+", "", full_desc)
    # å£“ç¸®å¤šé¤˜ç©ºç™½
    full_desc = re.sub(r"\s+", " ", full_desc).strip()
    
    return full_desc, country_val

def generate_sop(ccc, permit):
    """SOP é‚è¼¯"""
    notes = []
    p = str(permit)
    c = str(ccc).replace(".", "").replace("-", "")
    
    if 'IFB' in p: notes.append("é£Ÿå“å®¹å™¨ (Food Contact) - éœ€æª¢é©—")
    elif 'CI' in p: notes.append("ä¸€èˆ¬æŸ¥é©— (General Inspection)")
    elif 'DH' in p: notes.append("å¯èƒ½ç‚ºå…é©—æˆ–æ ¸å‚™ä»£ç¢¼")
    
    if c.startswith('9503'): notes.append("ç©å…· (Toys) - éœ€ BSMI æª¢é©—")
    if c.startswith('3924'): notes.append("å¡‘è† /ç¾è€çš¿æª¢é©—")
    if c.startswith('940'): notes.append("ç‡ˆå…·/å®¶å…· - æ³¨æ„æª¢é©—")
    if c.startswith('691'): notes.append("é™¶ç“·æª¢é©—")
    if c.startswith('9603'): notes.append("åˆ·å…· - æ³¨æ„å‹•ç‰©æ¯›/æ¤ç‰©æ¯›")
    if c.startswith('630') or c.startswith('570'): notes.append("ç´¡ç¹”å“ - æ³¨æ„æˆåˆ†æ¨™ç¤º")
    if c.startswith('910'): notes.append("é˜éŒ¶/è¨ˆæ™‚å™¨ - æ³¨æ„é›»æ± è¦å®š")
    
    return "ï¼›".join(notes)

# ==========================================
# 3. è§£æå¼•æ“ (V12.0)
# ==========================================

def parse_pdf_v12(pdf_path):
    print(f"ğŸš€ å•Ÿå‹• V12.0 è§£æ: {pdf_path}")
    
    items = [] 
    last_item_idx = None 
    
    with pdfplumber.open(pdf_path) as pdf:
        
        # 1. æŠ“å–å ±å–®è™Ÿ
        decl_no = "Unknown"
        p1_text = pdf.pages[0].extract_text() or ""
        decl_match = re.search(r"([A-Z]{2}/[\s\d/]+/[A-Z0-9]+)", p1_text)
        if decl_match: 
            decl_no = decl_match.group(1).replace(" ", "").replace("//", "/")

        # 2. éæ­·æ¯ä¸€é 
        for page_num, page in enumerate(pdf.pages):
            words = page.extract_words(keep_blank_chars=True)
            
            # --- A. é è™•ç†ï¼šéæ¿¾å…¨åŸŸé›œè¨Š ---
            valid_words = []
            for w in words:
                if not is_header_noise(w['text']):
                    valid_words.append(w)
            words = valid_words

            # --- B. æ‰¾å‡ºéŒ¨é» ---
            anchors = []
            for w in words:
                if w['x0'] < COORD_SPLIT_CCC: 
                    if re.match(r"^\d+\.$", w['text'].strip()):
                        item_num = int(w['text'].strip().replace(".", ""))
                        anchors.append({'item': item_num, 'top': w['top']})
            anchors.sort(key=lambda x: x['top'])
            
            # --- C. å®šç¾©å€å¡Š ---
            zones = []
            if anchors:
                first_anchor_top = anchors[0]['top']
                if first_anchor_top > 10: 
                    zones.append({'start_y': 0, 'end_y': first_anchor_top, 'item_id': last_item_idx})
            else:
                zones.append({'start_y': 0, 'end_y': page.height, 'item_id': last_item_idx})
            
            for i in range(len(anchors)):
                start_y = anchors[i]['top']
                end_y = anchors[i+1]['top'] if i < len(anchors) - 1 else page.height
                last_item_idx = anchors[i]['item']
                zones.append({'start_y': start_y, 'end_y': end_y, 'item_id': anchors[i]['item']})
            
            # --- D. æå–æ–‡å­— ---
            for zone in zones:
                z_item_id = zone['item_id']
                if z_item_id is None: continue 
                
                target_item = next((it for it in items if it['item_no'] == z_item_id), None)
                if not target_item:
                    target_item = {'item_no': z_item_id, 'desc_parts': [], 'ccc_parts': [], 'decl_no': decl_no}
                    items.append(target_item)
                
                zone_words = [w for w in words if zone['start_y'] <= w['top'] < zone['end_y']]
                zone_words.sort(key=lambda w: (round(w['top']/2), w['x0']))
                
                for w in zone_words:
                    x = w['x0']
                    text = w['text']
                    
                    # å·¦æ¬„: å“åèˆ‡é …æ¬¡
                    if COORD_DESC_MIN_X <= x < COORD_SPLIT_CCC:
                        if re.match(r"^\d+\.$", text.strip()): continue # è·³ééŒ¨é»æœ¬èº«
                        target_item['desc_parts'].append(text)
                        
                    # ä¸­æ¬„: ç¨…å‰‡èˆ‡è¨±å¯è­‰
                    elif COORD_SPLIT_CCC <= x < COORD_NOISE_START:
                        target_item['ccc_parts'].append(text)
                    
                    # å³æ¬„: é›œè¨Š -> ä¸Ÿæ£„

    # 4. è¼¸å‡ºèˆ‡å¾Œè™•ç†
    final_data = []
    items.sort(key=lambda x: x['item_no'])
    
    for it in items:
        # åˆ†é›¢ CCC èˆ‡ Permit
        ccc_val, permit_val = extract_ccc_permit(it['ccc_parts'])
        
        # åˆ†é›¢ Country èˆ‡ Description
        desc_val, country_val = extract_country_and_clean_desc(it['desc_parts'])
        
        # æŠ“å–æ¢ç¢¼ (å¾ desc_parts åŸå§‹åˆ—è¡¨è£¡æ‰¾æ¯”è¼ƒä¿éšªï¼Œé›–ç„¶ clean_desc å·²ç¶“ç§»é™¤äº†)
        barcode = ""
        full_raw_desc = " ".join(it['desc_parts'])
        bc_match = re.search(r"\b(\d{13})\b", full_raw_desc)
        if bc_match: barcode = bc_match.group(1)

        final_data.append({
            "å ±å–®è™Ÿç¢¼": it['decl_no'],
            "é …æ¬¡": it['item_no'],
            "è²¨è™Ÿ/æ¢ç¢¼": barcode,
            "è²¨ç‰©åç¨±": desc_val,
            "ç¨…å‰‡è™Ÿåˆ—": ccc_val,
            "è¨±å¯è­‰è™Ÿç¢¼": permit_val,
            "ç”Ÿç”¢åœ‹åˆ¥": country_val,
            "ç”³å ±æ³¨æ„äº‹é …": generate_sop(ccc_val, permit_val)
        })
        
    return final_data

# ==========================================
# 4. ä¸»ç¨‹å¼
# ==========================================
def main():
    if not os.path.exists(PDF_PATH):
        print(f"æª”æ¡ˆä¸å­˜åœ¨: {PDF_PATH}")
        return

    result = parse_pdf_v12(PDF_PATH)
    
    df = pd.DataFrame(result)
    cols = ['å ±å–®è™Ÿç¢¼', 'é …æ¬¡', 'è²¨è™Ÿ/æ¢ç¢¼', 'è²¨ç‰©åç¨±', 'ç¨…å‰‡è™Ÿåˆ—', 'è¨±å¯è­‰è™Ÿç¢¼', 'ç”Ÿç”¢åœ‹åˆ¥', 'ç”³å ±æ³¨æ„äº‹é …']
    df = df[cols]
    
    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… V12.0 è™•ç†å®Œæˆ! æª”æ¡ˆå·²å„²å­˜: {OUTPUT_CSV}")
    print(df.head().to_string())

if __name__ == "__main__":
    main()